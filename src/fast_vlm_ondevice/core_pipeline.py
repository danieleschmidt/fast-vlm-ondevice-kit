"""
Core pipeline implementation for FastVLM inference without external dependencies.

This module provides a lightweight implementation that demonstrates the architecture
and can run without PyTorch, CoreML, or other heavy dependencies.
"""

import json
import time
import logging
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
import base64
import hashlib
import threading
from threading import Lock

logger = logging.getLogger(__name__)


class EnhancedInputValidator:
    """Enhanced input validation with security and safety checks."""
    
    def __init__(self):
        # Import enhanced security framework (Generation 2+)
        try:
            from .enhanced_security_framework import create_enhanced_validator
            self.security_validator = create_enhanced_validator()
            # Enable enhanced security for Generation 2
            self.enhanced_security = True
            logger.info("üõ°Ô∏è Enhanced security framework enabled for Generation 2")
        except ImportError:
            self.enhanced_security = False
            logger.warning("Enhanced security framework not available, using basic validation")
            
        # Fallback validation parameters
        self.max_image_size_mb = 50  # 50MB max image size
        self.max_question_length = 1000  # Maximum question length
        self.blocked_patterns = [
            r'<script', r'javascript:', r'data:text/html',
            r'eval\s*\(', r'document\.', r'window\.'
        ]
        
    def validate_image(self, image_data: bytes) -> Tuple[bool, str]:
        """Validate image data for security and format."""
        try:
            # Use enhanced security if available
            if self.enhanced_security:
                # Enhanced validation includes comprehensive security scanning
                is_safe, message, incidents = self.security_validator.validate_request(
                    image_data, "validation_check"
                )
                if not is_safe:
                    return False, message
                
                # Additional size check
                size_mb = len(image_data) / (1024 * 1024)
                if size_mb > self.max_image_size_mb:
                    return False, f"Image too large: {size_mb:.1f}MB (max: {self.max_image_size_mb}MB)"
                
                return True, "Valid image data (enhanced security)"
            
            # Fallback basic validation
            size_mb = len(image_data) / (1024 * 1024)
            if size_mb > self.max_image_size_mb:
                return False, f"Image too large: {size_mb:.1f}MB (max: {self.max_image_size_mb}MB)"
            
            # Check for suspicious content
            data_str = str(image_data)[:1000]  # Check first 1KB
            for pattern in self.blocked_patterns:
                import re
                if re.search(pattern, data_str, re.IGNORECASE):
                    return False, f"Suspicious content detected in image data"
            
            # Basic format validation
            if len(image_data) < 100:
                return False, "Image data too small to be valid"
                
            return True, "Valid image data"
            
        except Exception as e:
            return False, f"Image validation error: {e}"
    
    def validate_question(self, question: str) -> Tuple[bool, str]:
        """Validate question text for safety and format."""
        try:
            # Use enhanced security if available
            if self.enhanced_security:
                # Create adequate dummy image for question-only validation
                dummy_image = b"validation_image_data" + b"x" * 1000  # Make it large enough
                is_safe, message, incidents = self.security_validator.validate_request(
                    dummy_image, question
                )
                if not is_safe:
                    return False, message
                return True, "Valid question (enhanced security)"
            
            # Fallback basic validation
            if len(question) > self.max_question_length:
                return False, f"Question too long: {len(question)} chars (max: {self.max_question_length})"
            
            if len(question.strip()) == 0:
                return False, "Question cannot be empty"
            
            # Check for suspicious patterns
            for pattern in self.blocked_patterns:
                import re
                if re.search(pattern, question, re.IGNORECASE):
                    return False, "Suspicious content detected in question"
            
            # Check for reasonable character set
            if not all(ord(c) < 1000000 for c in question):  # Basic Unicode range check
                return False, "Question contains invalid characters"
                
            return True, "Valid question"
            
        except Exception as e:
            return False, f"Question validation error: {e}"
    
    def get_security_status(self) -> Dict[str, Any]:
        """Get security status information."""
        if self.enhanced_security and hasattr(self.security_validator, 'get_security_status'):
            return self.security_validator.get_security_status()
        else:
            return {
                "enhanced_security": self.enhanced_security,
                "validation_mode": "basic" if not self.enhanced_security else "enhanced",
                "max_image_size_mb": self.max_image_size_mb,
                "max_question_length": self.max_question_length
            }


class CircuitBreaker:
    """Circuit breaker pattern for fault tolerance."""
    
    def __init__(self, failure_threshold: int = 5, timeout_seconds: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout_seconds = timeout_seconds
        self.failure_count = 0
        self.last_failure_time = 0
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
        self.lock = Lock()
    
    def call(self, func, *args, **kwargs):
        """Execute function with circuit breaker protection."""
        with self.lock:
            if self.state == "OPEN":
                if time.time() - self.last_failure_time > self.timeout_seconds:
                    self.state = "HALF_OPEN"
                    logger.info("Circuit breaker transitioning to HALF_OPEN")
                else:
                    raise RuntimeError("Circuit breaker is OPEN - system unavailable")
            
            try:
                result = func(*args, **kwargs)
                if self.state == "HALF_OPEN":
                    self._reset()
                return result
                
            except Exception as e:
                self._record_failure()
                raise e
    
    def _record_failure(self):
        """Record a failure and update circuit breaker state."""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"
            logger.warning(f"Circuit breaker OPENED after {self.failure_count} failures")
    
    def _reset(self):
        """Reset circuit breaker to closed state."""
        self.failure_count = 0
        self.state = "CLOSED"
        logger.info("Circuit breaker RESET to CLOSED state")


@dataclass
class InferenceConfig:
    """Configuration for VLM inference."""
    model_name: str = "fast-vlm-base"
    max_sequence_length: int = 77
    image_size: Tuple[int, int] = (336, 336)
    quantization_bits: int = 4
    enable_caching: bool = True
    timeout_seconds: float = 30.0
    batch_size: int = 1


@dataclass
class InferenceResult:
    """Result from VLM inference."""
    answer: str
    confidence: float
    latency_ms: float
    model_used: str
    timestamp: str
    metadata: Dict[str, Any]


class MockVisionEncoder:
    """Mock vision encoder for demonstration purposes."""
    
    def __init__(self, model_size: str = "base"):
        self.model_size = model_size
        self.feature_dim = {"tiny": 256, "base": 512, "large": 768}[model_size]
        logger.info(f"Initialized MockVisionEncoder ({model_size}, {self.feature_dim}d)")
    
    def encode_image(self, image_data: bytes) -> Dict[str, Any]:
        """Encode image to feature representation."""
        # Simulate image encoding with deterministic features
        import hashlib
        image_hash = hashlib.sha256(image_data).hexdigest()
        
        # Generate mock features based on image hash (deterministic)
        features = [
            (int(image_hash[i:i+2], 16) / 255.0 - 0.5) * 2  # Normalize to [-1, 1]
            for i in range(0, min(len(image_hash), self.feature_dim * 2), 2)
        ]
        
        # Pad or truncate to feature_dim
        if len(features) < self.feature_dim:
            features.extend([0.0] * (self.feature_dim - len(features)))
        else:
            features = features[:self.feature_dim]
        
        return {
            "features": features,
            "spatial_features": [features[:64], features[64:128]] if len(features) >= 128 else [features],
            "attention_mask": [1.0] * min(64, len(features)),
            "image_hash": image_hash
        }


class MockTextEncoder:
    """Mock text encoder for demonstration purposes."""
    
    def __init__(self, vocab_size: int = 50000):
        self.vocab_size = vocab_size
        self.embedding_dim = 512
        logger.info(f"Initialized MockTextEncoder (vocab={vocab_size}, dim={self.embedding_dim})")
    
    def encode_text(self, text: str) -> Dict[str, Any]:
        """Encode text to feature representation."""
        # Simple tokenization simulation
        tokens = text.lower().split()
        
        # Generate mock embeddings based on text content
        text_features = []
        for token in tokens[:20]:  # Max 20 tokens
            token_hash = hash(token) % 1000
            embedding = [(token_hash + i) / 1000.0 for i in range(self.embedding_dim)]
            text_features.append(embedding)
        
        # Pad to fixed length
        max_length = 20
        while len(text_features) < max_length:
            text_features.append([0.0] * self.embedding_dim)
        
        return {
            "token_embeddings": text_features[:max_length],
            "attention_mask": [1.0] * min(len(tokens), max_length) + [0.0] * max(0, max_length - len(tokens)),
            "sequence_length": min(len(tokens), max_length),
            "original_text": text
        }


class MockFusionModule:
    """Mock cross-modal fusion module."""
    
    def __init__(self, vision_dim: int = 512, text_dim: int = 512):
        self.vision_dim = vision_dim
        self.text_dim = text_dim
        self.fusion_dim = 768
        logger.info(f"Initialized MockFusionModule (vision={vision_dim}, text={text_dim}, fusion={self.fusion_dim})")
    
    def fuse_modalities(self, vision_features: Dict[str, Any], text_features: Dict[str, Any]) -> Dict[str, Any]:
        """Fuse vision and text features."""
        # Simulate cross-attention fusion
        v_features = vision_features["features"]
        t_features = text_features["token_embeddings"][0]  # Use first token embedding
        
        # Simple fusion: concatenate and project
        fused_features = []
        for i in range(min(len(v_features), len(t_features))):
            fused_val = (v_features[i] + t_features[i]) / 2.0
            fused_features.append(fused_val)
        
        # Simulate attention scores
        attention_scores = [abs(f) for f in fused_features[:10]]
        
        return {
            "fused_features": fused_features,
            "cross_attention": attention_scores,
            "fusion_quality": sum(attention_scores) / len(attention_scores) if attention_scores else 0.0
        }


class MockAnswerGenerator:
    """Mock answer generation module."""
    
    def __init__(self):
        self.common_responses = {
            "what": ["objects", "items", "things", "elements"],
            "how": ["process", "method", "way", "approach"], 
            "where": ["location", "place", "position", "area"],
            "when": ["time", "moment", "period", "duration"],
            "who": ["person", "people", "individual", "character"],
            "why": ["reason", "cause", "purpose", "explanation"]
        }
        logger.info("Initialized MockAnswerGenerator")
    
    def generate_answer(self, fused_features: Dict[str, Any], question: str) -> str:
        """Generate answer based on fused features and question."""
        question_lower = question.lower()
        
        # Simple rule-based generation
        if any(word in question_lower for word in ["what", "object", "thing"]):
            confidence = fused_features.get("fusion_quality", 0.5)
            if confidence > 0.7:
                return "I can see several objects including furniture, decorative items, and possibly electronic devices in the image."
            elif confidence > 0.4:
                return "There appear to be some objects in the image, but I need better image quality for specific identification."
            else:
                return "I can detect some items but cannot clearly identify specific objects."
        
        elif any(word in question_lower for word in ["color", "colour"]):
            return "The image contains various colors including what appears to be neutral tones with some accent colors."
        
        elif any(word in question_lower for word in ["person", "people", "human"]):
            return "I can analyze the image for people, but I prioritize privacy and provide general descriptions only."
        
        elif any(word in question_lower for word in ["count", "how many", "number"]):
            attention_scores = fused_features.get("cross_attention", [])
            count = len([s for s in attention_scores if s > 0.5])
            return f"Based on my analysis, I can identify approximately {count} distinct elements or regions of interest."
        
        elif any(word in question_lower for word in ["describe", "scene", "setting"]):
            return "This appears to be an indoor scene with various objects and elements arranged in what looks like a lived-in space."
        
        else:
            return "I can see various elements in the image. Could you ask a more specific question about what you'd like to know?"


class FastVLMCorePipeline:
    """Core FastVLM inference pipeline without external dependencies."""
    
    def __init__(self, config: Optional[InferenceConfig] = None):
        """Initialize the core pipeline."""
        self.config = config or InferenceConfig()
        
        # Enhanced error tracking and metrics
        self.processing_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "cache_hits": 0,
            "error_count": 0,
            "average_latency_ms": 0.0,
            "last_error": None,
            "startup_time": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # Circuit breaker for fault tolerance
        self.circuit_breaker = CircuitBreaker(failure_threshold=5, timeout_seconds=60)
        
        # Input validation
        self.input_validator = EnhancedInputValidator()
        
        # Thread safety
        self.processing_lock = Lock()
        
        # Initialize mock components with error handling
        try:
            model_size = self._determine_model_size()
            self.vision_encoder = MockVisionEncoder(model_size)
            self.text_encoder = MockTextEncoder()
            self.fusion_module = MockFusionModule()
            self.answer_generator = MockAnswerGenerator()
            
            # Enhanced cache with size management
            self.cache = {} if self.config.enable_caching else None
            self.cache_stats = {"hits": 0, "misses": 0, "evictions": 0}
            
            # Initialize mobile performance optimizer
            try:
                from .mobile_performance_optimizer import create_mobile_optimizer
                self.mobile_optimizer = create_mobile_optimizer(
                    max_memory_mb=256,  # Conservative for mobile
                    enable_batching=False,  # Disabled for single-user scenarios
                    enable_adaptive_quality=True
                )
                self.mobile_optimizer.initialize(self)
                logger.info("üöÄ Generation 3: Advanced mobile performance optimizer enabled")
            except Exception as e:
                logger.warning(f"Mobile optimizer disabled: {e}")
                self.mobile_optimizer = None
            
            # Initialize hyper scaling engine (Generation 3)
            try:
                from .hyper_scaling_engine import create_hyper_scaling_engine, ScalingStrategy
                self.scaling_engine = create_hyper_scaling_engine(
                    min_workers=1,
                    max_workers=8,  # Increased for Generation 3
                    cache_l1_size=100,  # Enhanced caching
                    cache_l2_size=500,
                    strategy=ScalingStrategy.ADAPTIVE,
                    enable_gpu_acceleration=True,
                    enable_mobile_optimization=True
                )
                self.scaling_enabled = True
                logger.info("üöÄ Generation 3: Hyper scaling engine enabled with GPU acceleration")
            except Exception as e:
                logger.warning(f"Scaling engine disabled: {e}")
                self.scaling_engine = None
                self.scaling_enabled = False
            
            # Initialize quantum optimization engine (Generation 3)
            try:
                from .quantum_optimization import create_quantum_optimizer
                self.quantum_optimizer = create_quantum_optimizer(
                    enable_variational_optimization=True,
                    enable_quantum_annealing=True,
                    hybrid_classical_quantum=True
                )
                logger.info("üåå Generation 3: Quantum optimization engine enabled")
                self.quantum_enabled = True
            except Exception as e:
                logger.debug(f"Quantum optimization not available: {e}")
                self.quantum_optimizer = None
                self.quantum_enabled = False
            
            # Initialize neuromorphic computing (Generation 3)
            try:
                from .neuromorphic import create_neuromorphic_config
                self.neuromorphic_config = create_neuromorphic_config(
                    spike_threshold=0.7,
                    temporal_integration=True,
                    synaptic_plasticity=True
                )
                logger.info("üß† Generation 3: Neuromorphic computing integration enabled")
                self.neuromorphic_enabled = True
            except Exception as e:
                logger.debug(f"Neuromorphic computing not available: {e}")
                self.neuromorphic_config = None
                self.neuromorphic_enabled = False
            
            logger.info(f"‚úÖ FastVLMCorePipeline initialized successfully with {self.config.model_name} (Generation 3)")
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline initialization failed: {e}")
            self._initialize_fallback_components()
            raise RuntimeError(f"Pipeline initialization failed: {e}")
    
    def _initialize_fallback_components(self):
        """Initialize basic fallback components when main initialization fails."""
        logger.warning("üö® Initializing fallback components")
        self.vision_encoder = MockVisionEncoder("tiny")
        self.text_encoder = MockTextEncoder(vocab_size=1000)
        self.fusion_module = MockFusionModule(vision_dim=256, text_dim=256)
        self.answer_generator = MockAnswerGenerator()
        self.cache = None  # Disable caching in fallback mode
    
    def _determine_model_size(self) -> str:
        """Determine model size from config."""
        if "tiny" in self.config.model_name:
            return "tiny"
        elif "large" in self.config.model_name:
            return "large"
        else:
            return "base"
    
    def _generate_cache_key(self, image_data: bytes, question: str) -> str:
        """Generate cache key for image-question pair."""
        import hashlib
        combined = base64.b64encode(image_data).decode() + question
        return hashlib.sha256(combined.encode()).hexdigest()[:16]
    
    def process_image_question(self, image_data: bytes, question: str) -> InferenceResult:
        """Process image and question to generate answer with enhanced error handling."""
        # Use scaling engine if available (Generation 3)
        if self.scaling_enabled and self.scaling_engine:
            try:
                # Create processing function compatible with scaling engine
                def processing_func(img_data, quest):
                    return self._process_image_question_internal(img_data, quest)
                
                result_dict = self.scaling_engine.process_request_scaled(
                    image_data, question, processing_func
                )
                # Convert dict back to InferenceResult
                if isinstance(result_dict, dict):
                    return InferenceResult(**result_dict)
                else:
                    return result_dict
            except Exception as e:
                logger.warning(f"Generation 3 scaling engine failed, falling back to standard processing: {e}")
        
        # Standard processing (Generations 1 & 2) - With reliability features for Generation 2
        with self.processing_lock:
            return self.circuit_breaker.call(self._process_image_question_internal, image_data, question)
    
    def _process_image_question_internal(self, image_data: bytes, question: str) -> InferenceResult:
        """Internal processing method with comprehensive validation and error handling."""
        start_time = time.time()
        self.processing_stats["total_requests"] += 1
        
        try:
            # Enhanced input validation with debugging
            logger.debug(f"Validating image of size {len(image_data)} bytes")
            image_valid, image_msg = self.input_validator.validate_image(image_data)
            logger.debug(f"Image validation result: {image_valid}, message: {image_msg}")
            if not image_valid:
                raise ValueError(f"Image validation failed: {image_msg}")
            
            logger.debug(f"Validating question: {question[:50]}...")
            question_valid, question_msg = self.input_validator.validate_question(question)
            logger.debug(f"Question validation result: {question_valid}, message: {question_msg}")
            if not question_valid:
                raise ValueError(f"Question validation failed: {question_msg}")
            
            # Check cache first
            cache_key = self._generate_cache_key(image_data, question) if self.cache is not None else None
            if cache_key and cache_key in self.cache:
                cached_result = self.cache[cache_key]
                self.processing_stats["cache_hits"] += 1
                self.cache_stats["hits"] += 1
                logger.info(f"üéØ Cache hit for key {cache_key[:8]}...")
                
                # Update cache access time
                cached_result["metadata"]["cache_used"] = True
                cached_result["metadata"]["access_time"] = time.strftime("%Y-%m-%d %H:%M:%S")
                
                return InferenceResult(**cached_result)
            
            # Record cache miss
            if cache_key:
                self.cache_stats["misses"] += 1
            
            # Step 1: Encode image with error handling
            try:
                vision_features = self.vision_encoder.encode_image(image_data)
                logger.debug(f"üñºÔ∏è Vision encoding completed: {len(vision_features['features'])}d features")
            except Exception as e:
                raise RuntimeError(f"Vision encoding failed: {e}")
            
            # Step 2: Encode text with error handling
            try:
                text_features = self.text_encoder.encode_text(question)
                logger.debug(f"üìù Text encoding completed: {text_features['sequence_length']} tokens")
            except Exception as e:
                raise RuntimeError(f"Text encoding failed: {e}")
            
            # Step 3: Fuse modalities with error handling
            try:
                fused_features = self.fusion_module.fuse_modalities(vision_features, text_features)
                logger.debug(f"üîó Fusion completed: quality={fused_features.get('fusion_quality', 0):.3f}")
            except Exception as e:
                raise RuntimeError(f"Multimodal fusion failed: {e}")
            
            # Step 4: Generate answer with error handling
            try:
                answer = self.answer_generator.generate_answer(fused_features, question)
                if not answer or len(answer.strip()) == 0:
                    answer = "I apologize, but I couldn't generate a meaningful response to your question."
                logger.debug(f"üí¨ Answer generated: {len(answer)} characters")
            except Exception as e:
                raise RuntimeError(f"Answer generation failed: {e}")
            
            # Calculate comprehensive metrics
            latency_ms = (time.time() - start_time) * 1000
            confidence = max(0.0, min(1.0, fused_features.get("fusion_quality", 0.5)))  # Clamp to [0,1]
            
            # Update processing statistics
            self.processing_stats["successful_requests"] += 1
            self._update_average_latency(latency_ms)
            
            # Create enhanced result
            result = InferenceResult(
                answer=answer,
                confidence=confidence,
                latency_ms=latency_ms,
                model_used=self.config.model_name,
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                metadata={
                    "vision_features_dim": len(vision_features["features"]),
                    "text_tokens": text_features["sequence_length"],
                    "fusion_dim": len(fused_features["fused_features"]),
                    "image_hash": vision_features["image_hash"],
                    "cache_used": False,
                    "processing_time_breakdown": {
                        "total_ms": latency_ms,
                        "validation_ms": 1.0,  # Estimated
                        "vision_encoding_ms": latency_ms * 0.3,
                        "text_encoding_ms": latency_ms * 0.2,
                        "fusion_ms": latency_ms * 0.3,
                        "generation_ms": latency_ms * 0.2
                    },
                    "cache_stats": dict(self.cache_stats),
                    "request_id": self._generate_request_id()
                }
            )
            
            # Enhanced cache management with size limits
            if cache_key and self.cache is not None:
                self._manage_cache_size()
                self.cache[cache_key] = asdict(result)
                logger.info(f"üíæ Cached result for key {cache_key[:8]}... (cache size: {len(self.cache)})")
            
            logger.info(f"‚úÖ Processing completed successfully: {latency_ms:.1f}ms, confidence: {confidence:.3f}")
            
            # Generation 2: Enhanced monitoring and alerts
            if confidence < 0.3:
                logger.warning(f"‚ö†Ô∏è Low confidence result: {confidence:.3f} - Consider model retraining")
            if latency_ms > 500:  # Alert if >500ms (well above 250ms target)
                logger.warning(f"üêå High latency detected: {latency_ms:.1f}ms - Performance degradation possible")
                
            # Generation 2: Quality metrics tracking
            self.processing_stats["confidence_sum"] = self.processing_stats.get("confidence_sum", 0) + confidence
            self.processing_stats["avg_confidence"] = (
                self.processing_stats["confidence_sum"] / self.processing_stats["successful_requests"]
            )
            return result
            
        except Exception as e:
            # Generation 2: Enhanced error handling and recovery
            self.processing_stats["error_count"] += 1
            self.processing_stats["last_error"] = str(e)
            error_latency = (time.time() - start_time) * 1000
            
            # Classify error type for better recovery
            error_type = type(e).__name__
            
            if "validation" in str(e).lower():
                logger.error(f"‚ùå Input validation failed: {e}")
                recovery_msg = "Please check your input format and try again."
            elif "memory" in str(e).lower() or "resource" in str(e).lower():
                logger.error(f"‚ùå Resource exhaustion: {e}")
                recovery_msg = "System resources temporarily unavailable. Please try again."
                # Trigger cache cleanup
                if self.cache:
                    self._emergency_cache_cleanup()
            elif "timeout" in str(e).lower():
                logger.error(f"‚ùå Processing timeout: {e}")
                recovery_msg = "Processing took too long. Please try with simpler input."
            else:
                logger.error(f"‚ùå Unexpected error: {e}")
                recovery_msg = "An unexpected error occurred. Please contact support if this persists."
            
            # Return enhanced error response
            return InferenceResult(
                answer=f"I couldn't process your request due to {error_type.lower()}. {recovery_msg}",
                confidence=0.0,
                latency_ms=error_latency,
                model_used=self.config.model_name,
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                metadata={
                    "error": str(e),
                    "error_type": error_type,
                    "error_occurred_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "fallback_response": True,
                    "request_id": self._generate_request_id(),
                    "recovery_action": recovery_msg,
                    "error_context": {
                        "input_validated": hasattr(self, '_validation_passed'),
                        "model_loaded": hasattr(self, 'vision_encoder'),
                        "cache_size": len(self.cache) if self.cache else 0
                    }
                }
            )
    
    def process_text_only(self, question: str) -> InferenceResult:
        """Process text-only question (no image)."""
        start_time = time.time()
        
        try:
            # Create dummy image data for consistency
            dummy_image = b"dummy_image_data"
            
            # Use general knowledge responses
            answer = self._generate_general_response(question)
            
            latency_ms = (time.time() - start_time) * 1000
            
            return InferenceResult(
                answer=answer,
                confidence=0.8,  # High confidence for general knowledge
                latency_ms=latency_ms,
                model_used=self.config.model_name,
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                metadata={
                    "mode": "text_only",
                    "question_length": len(question),
                    "response_type": "general_knowledge"
                }
            )
            
        except Exception as e:
            logger.error(f"Text-only processing failed: {e}")
            return InferenceResult(
                answer=f"Error processing text question: {str(e)}",
                confidence=0.0,
                latency_ms=(time.time() - start_time) * 1000,
                model_used=self.config.model_name,
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                metadata={"error": str(e)}
            )
    
    def _generate_general_response(self, question: str) -> str:
        """Generate general knowledge response."""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ["hello", "hi", "greeting"]):
            return "Hello! I'm FastVLM, a vision-language model. I can analyze images and answer questions about them."
        
        elif any(word in question_lower for word in ["help", "what can you do"]):
            return "I can analyze images and answer questions about what I see, including objects, scenes, colors, and spatial relationships."
        
        elif any(word in question_lower for word in ["fastVLM", "fast vlm", "model"]):
            return "FastVLM is an efficient vision-language model optimized for mobile devices, capable of <250ms inference on modern smartphones."
        
        else:
            return "I'm designed to analyze images and answer questions about them. Please provide an image along with your question for the best results."
    
    def get_stats(self) -> Dict[str, Any]:
        """Get pipeline statistics."""
        cache_size = len(self.cache) if self.cache else 0
        
        return {
            "model_name": self.config.model_name,
            "cache_enabled": self.config.enable_caching,
            "cache_entries": cache_size,
            "quantization_bits": self.config.quantization_bits,
            "max_sequence_length": self.config.max_sequence_length,
            "image_size": self.config.image_size,
            "components": {
                "vision_encoder": self.vision_encoder.model_size,
                "text_encoder": f"{self.text_encoder.vocab_size} vocab",
                "fusion_module": f"{self.fusion_module.fusion_dim}d",
                "answer_generator": "rule_based"
            }
        }
    
    def clear_cache(self) -> int:
        """Clear inference cache and return number of entries cleared."""
        if self.cache:
            entries = len(self.cache)
            self.cache.clear()
            logger.info(f"Cleared {entries} cache entries")
            return entries
        return 0
    
    def _update_average_latency(self, new_latency_ms: float):
        """Update running average latency."""
        current_avg = self.processing_stats["average_latency_ms"]
        total_requests = self.processing_stats["successful_requests"]
        
        if total_requests == 1:
            self.processing_stats["average_latency_ms"] = new_latency_ms
        else:
            # Weighted average calculation
            self.processing_stats["average_latency_ms"] = (
                (current_avg * (total_requests - 1) + new_latency_ms) / total_requests
            )
    
    def _generate_request_id(self) -> str:
        """Generate unique request ID for tracking."""
        import uuid
        return str(uuid.uuid4())[:8]
    
    def _manage_cache_size(self, max_cache_size: int = 100):
        """Manage cache size with LRU eviction."""
        if self.cache and len(self.cache) >= max_cache_size:
            # Simple LRU: remove oldest 20% of entries
            items_to_remove = max(1, len(self.cache) // 5)
            oldest_keys = list(self.cache.keys())[:items_to_remove]
            
            for key in oldest_keys:
                del self.cache[key]
                self.cache_stats["evictions"] += 1
            
            logger.info(f"üóëÔ∏è Cache evicted {items_to_remove} old entries")
    
    def _generate_error_response(self, error_message: str) -> str:
        """Generate user-friendly error response."""
        if "validation failed" in error_message.lower():
            return "I couldn't process your request due to input validation issues. Please check your image and question format."
        elif "encoding failed" in error_message.lower():
            return "I encountered an issue while processing your image or question. Please try again with a different format."
        elif "fusion failed" in error_message.lower():
            return "I had trouble understanding the relationship between your image and question. Please try rephrasing your question."
        elif "generation failed" in error_message.lower():
            return "I couldn't generate a response to your question. Please try asking in a different way."
        elif "circuit breaker" in error_message.lower():
            return "The system is temporarily unavailable due to high error rates. Please try again in a few moments."
        else:
            return "I encountered an unexpected issue while processing your request. Please try again."
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get comprehensive health status of the pipeline."""
        total_requests = self.processing_stats["total_requests"]
        successful_requests = self.processing_stats["successful_requests"]
        
        success_rate = (successful_requests / total_requests * 100) if total_requests > 0 else 0
        
        # Get security status if available
        security_status = {}
        if hasattr(self.input_validator, 'get_security_status'):
            try:
                security_status = self.input_validator.get_security_status()
            except:
                security_status = {"enhanced_security": False}
        
        base_status = {
            "status": "healthy" if success_rate >= 95 else "degraded" if success_rate >= 80 else "unhealthy",
            "success_rate_percent": round(success_rate, 2),
            "circuit_breaker_state": self.circuit_breaker.state,
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "error_count": self.processing_stats["error_count"],
            "cache_hit_rate": round(
                (self.cache_stats["hits"] / (self.cache_stats["hits"] + self.cache_stats["misses"]) * 100)
                if (self.cache_stats["hits"] + self.cache_stats["misses"]) > 0 else 0, 2
            ),
            "average_latency_ms": round(self.processing_stats["average_latency_ms"], 1),
            "last_error": self.processing_stats["last_error"],
            "uptime_since": self.processing_stats["startup_time"]
        }
        
        # Merge security status
        if security_status:
            base_status["security"] = security_status
        
        # Add scaling engine status (Generation 3)
        if self.scaling_enabled and self.scaling_engine:
            try:
                scaling_report = self.scaling_engine.get_performance_report()
                base_status["scaling"] = {
                    "enabled": True,
                    "workers": scaling_report["worker_pool"]["current_workers"],
                    "cache_hit_rate": scaling_report["cache_performance"]["overall_hit_rate_percent"],
                    "total_scaled": scaling_report["engine_stats"]["total_scaled"],
                    "auto_scaling": scaling_report["auto_scaling_enabled"]
                }
            except Exception as e:
                base_status["scaling"] = {"enabled": False, "error": str(e)}
        else:
            base_status["scaling"] = {"enabled": False}
        
        return base_status
    
    def _process_image_question_scaled(self, image_data: bytes, question: str) -> Dict[str, Any]:
        """Processing function for scaling engine (returns dict instead of InferenceResult)."""
        result = self._process_image_question_internal(image_data, question)
        
        # Convert InferenceResult to dict
        if hasattr(result, '__dict__'):
            return asdict(result) if hasattr(result, '_fields') else vars(result)
        else:
            return result

    def _emergency_cache_cleanup(self):
        """Emergency cache cleanup to free memory."""
        if not self.cache:
            return
        
        cache_size_before = len(self.cache)
        # Remove oldest 50% of entries
        sorted_keys = list(self.cache.keys())
        keys_to_remove = sorted_keys[:len(sorted_keys)//2]
        
        for key in keys_to_remove:
            del self.cache[key]
        
        self.cache_stats["evictions"] += len(keys_to_remove)
        logger.warning(f"üßπ Emergency cache cleanup: removed {len(keys_to_remove)} entries, {len(self.cache)} remaining")

    def _generate_request_id(self) -> str:
        """Generate unique request ID for tracing."""
        import uuid
        return str(uuid.uuid4())[:8]


# Convenience function for quick inference
def quick_inference(image_data: bytes, question: str, model_name: str = "fast-vlm-base") -> Dict[str, Any]:
    """Quick inference function for simple use cases."""
    config = InferenceConfig(model_name=model_name)
    pipeline = FastVLMCorePipeline(config)
    result = pipeline.process_image_question(image_data, question)
    return asdict(result)


# Demo data generator
def create_demo_image() -> bytes:
    """Create demo image data for testing that passes all validation."""
    # Generate clean demo image data optimized for enhanced security
    width, height, channels = 224, 224, 3
    
    # Use ultra-clean format that passes enhanced security validation
    metadata = f"DEMO_IMAGE_WIDTH{width}_HEIGHT{height}_CHANNELS{channels}_"
    
    # Generate safe pixel data without any patterns that could trigger security rules
    safe_chars = "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
    pixel_data = ""
    for i in range(width * height // 50):  # Generate enough data
        pixel_data += safe_chars[i % len(safe_chars)]
    
    # Add safe padding to ensure size requirements
    padding = "SAFE" * 500
    
    demo_data = (metadata + pixel_data + padding).encode()
    
    # Ensure size is adequate (>10KB to pass all checks)
    if len(demo_data) < 10000:
        demo_data += b"CLEAN" * ((10000 - len(demo_data)) // 5 + 1)
    
    return demo_data


if __name__ == "__main__":
    # Demo usage
    print("FastVLM Core Pipeline Demo")
    print("=" * 30)
    
    # Create pipeline
    config = InferenceConfig(model_name="fast-vlm-base", enable_caching=True)
    pipeline = FastVLMCorePipeline(config)
    
    # Demo image and questions
    demo_image = create_demo_image()
    demo_questions = [
        "What objects are in this image?",
        "What colors do you see?",
        "Describe the scene",
        "How many items are visible?"
    ]
    
    print(f"\nProcessing {len(demo_questions)} questions...")
    
    for i, question in enumerate(demo_questions, 1):
        print(f"\n{i}. Question: {question}")
        result = pipeline.process_image_question(demo_image, question)
        print(f"   Answer: {result.answer}")
        print(f"   Confidence: {result.confidence:.2f}")
        print(f"   Latency: {result.latency_ms:.1f}ms")
    
    # Show stats
    print(f"\nPipeline Stats:")
    stats = pipeline.get_stats()
    for key, value in stats.items():
        if isinstance(value, dict):
            print(f"  {key}:")
            for k, v in value.items():
                print(f"    {k}: {v}")
        else:
            print(f"  {key}: {value}")